{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import ast\n",
    "import json\n",
    "import re\n",
    "from pathlib import Path\n",
    "from typing import List, Optional, Tuple\n",
    "\n",
    "import vertexai\n",
    "from google.genai import types\n",
    "from vertexai.generative_models import (\n",
    "    Content,\n",
    "    GenerationConfig,\n",
    "    GenerationResponse,\n",
    "    GenerativeModel,\n",
    "    Part,\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Prompt Format"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Prompts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(\"/mnt/mydisk/Projects/plu/references/system_prompt.txt\", \"r\") as f:\n",
    "    system_prompt: str = f.read()\n",
    "with open(\"/mnt/mydisk/Projects/plu/references/user_prompt.txt\", \"r\") as f:\n",
    "    user_message: str = f.read()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Functions"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Format Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def split_markdown_by_images(markdown_text: str) -> Tuple[List[str], List[str]]:\n",
    "    \"\"\"\n",
    "    Divides a markdown text into segments and image tags.\n",
    "\n",
    "    Args:\n",
    "        markdown_text: The markdown text to split.\n",
    "\n",
    "    Returns:\n",
    "        A tuple containing the segments and image tags.\n",
    "    \"\"\"\n",
    "    # Motif regex pour capturer les balises d'images markdown : ![texte alternatif](chemin_image)\n",
    "    pattern = r\"(!\\[.*?\\]\\(.*?\\))\"\n",
    "    segments = re.split(pattern=pattern, string=markdown_text)\n",
    "    image_tags = re.findall(pattern=pattern, string=markdown_text)\n",
    "    return segments, image_tags\n",
    "\n",
    "\n",
    "def extract_image_filename(image_tag: str) -> Optional[str]:\n",
    "    \"\"\"\n",
    "    Extracts the image filename from an image tag.\n",
    "\n",
    "    Args:\n",
    "        image_tag: The image tag to extract the filename from.\n",
    "\n",
    "    Returns:\n",
    "        The image filename.\n",
    "    \"\"\"\n",
    "    match = re.search(r\"\\]\\((.*?)\\)\", image_tag)\n",
    "    if match:\n",
    "        return match.group(1)\n",
    "    return None\n",
    "\n",
    "\n",
    "def get_plu_ocr_data(user_message: str, path_dir: Path) -> List[any]:\n",
    "    \"\"\"\n",
    "    Document data for the generative model.\n",
    "\n",
    "    Args:\n",
    "        user_message: The user message.\n",
    "        path_dir: The path to the directory containing the user message.\n",
    "\n",
    "    Returns:\n",
    "        The user message and the parts of the generative model.\n",
    "    \"\"\"\n",
    "    md_file = next(path_dir.glob(\"*md\"))\n",
    "    with open(md_file, \"r\") as f:\n",
    "        document_content = f.read()\n",
    "\n",
    "    # Format the user message with document content\n",
    "    # Normalize newlines instead of completely removing them\n",
    "    user_message = user_message.format(DOCUMENT_CONTENT=document_content)\n",
    "\n",
    "    segments_message, image_tags = split_markdown_by_images(user_message)\n",
    "    images_dir = path_dir / Path(\"images\")\n",
    "    final_parts: List[any] = []\n",
    "\n",
    "    for segment in segments_message:\n",
    "        if segment in image_tags:\n",
    "            try:\n",
    "                image_filename = extract_image_filename(segment)\n",
    "                if image_filename:  # Only process if we have a valid filename\n",
    "                    image_path = images_dir / Path(image_filename)\n",
    "                    with open(image_path, \"rb\") as f:\n",
    "                        image_bytes = f.read()\n",
    "                        final_parts.append(\n",
    "                            Part.from_data(data=image_bytes, mime_type=\"image/jpeg\")\n",
    "                        )\n",
    "            except FileNotFoundError:\n",
    "                continue\n",
    "        elif segment.strip():  # Only add non-empty text segments\n",
    "            final_parts.append(Part.from_text(segment))\n",
    "\n",
    "    return final_parts"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## API Call"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_analysis(\n",
    "    user_message: str,\n",
    "    path_dir: Path,\n",
    "    system_prompt: Optional[str] = None,\n",
    ") -> GenerationResponse:\n",
    "    \"\"\"\n",
    "    Generate the analysis of the PLU document.\n",
    "    \n",
    "    Args:\n",
    "        user_message: The user message.\n",
    "        path_dir: The path to the directory containing the user message.\n",
    "        system_prompt: The system prompt.\n",
    "        \n",
    "    Returns:\n",
    "        The generation response.\n",
    "    \"\"\"\n",
    "    # Initialiser le client\n",
    "    vertexai.init(\n",
    "        project=\"analyse-plu\",\n",
    "        location=\"europe-west1\",\n",
    "    )\n",
    "\n",
    "    # Créer le modèle\n",
    "    model = GenerativeModel(\"gemini-2.0-flash-001\")\n",
    "\n",
    "    # Configuration de génération\n",
    "    generation_config = GenerationConfig(\n",
    "        temperature=1,\n",
    "        top_p=0.95,\n",
    "        max_output_tokens=8192,\n",
    "        # Les modalités de réponse\n",
    "        # response_mime_type=\"application/json\",\n",
    "        # response_schema={\n",
    "        #     \"type\": \"OBJECT\",\n",
    "        #     \"properties\": {\"response\": {\"type\": \"STRING\"}},\n",
    "        # },\n",
    "    )\n",
    "\n",
    "    # Paramètres de sécurité\n",
    "    # safety_settings = [\n",
    "    #     types.SafetySetting(category=\"HARM_CATEGORY_HATE_SPEECH\", threshold=\"OFF\"),\n",
    "    #     types.SafetySetting(\n",
    "    #         category=\"HARM_CATEGORY_DANGEROUS_CONTENT\", threshold=\"OFF\"\n",
    "    #     ),\n",
    "    #     types.SafetySetting(\n",
    "    #         category=\"HARM_CATEGORY_SEXUALLY_EXPLICIT\", threshold=\"OFF\"\n",
    "    #     ),\n",
    "    #     types.SafetySetting(category=\"HARM_CATEGORY_HARASSMENT\", threshold=\"OFF\"),\n",
    "    # ]\n",
    "\n",
    "    parts = get_plu_ocr_data(user_message=user_message, path_dir=path_dir)\n",
    "    contents = [\n",
    "        # Content(role=\"system\", parts=[Part.from_text(system_prompt)]), NOTE: Sytem prompt not supported w/ gemini-2.0-flash-001\n",
    "        Content(role=\"user\", parts=[Part.from_text(system_prompt)] + parts),\n",
    "    ]\n",
    "\n",
    "    # Générer la réponse\n",
    "    response: GenerationResponse = model.generate_content(\n",
    "        contents=contents,\n",
    "        generation_config=generation_config,\n",
    "        # safety_settings=safety_settings,\n",
    "        stream=False,\n",
    "    )\n",
    "\n",
    "    return response"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Save"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def save_generation_response(response: GenerationResponse, path: Path) -> dict:\n",
    "    \"\"\"\n",
    "    Save the generation response to a file.\n",
    "\n",
    "    Args:\n",
    "        response: The generation response.\n",
    "        path: The path to save the generation response.\n",
    "    \n",
    "    Returns:\n",
    "        The generation response data.\n",
    "    \"\"\"\n",
    "    response_data = response.to_dict()\n",
    "\n",
    "    # for i, candidate in enumerate(response_data[\"candidates\"]):\n",
    "    #     content = candidate[\"content\"]\n",
    "    #     for j, part in enumerate(content[\"parts\"]):\n",
    "            # text_dict = ast.literal_eval(part.get(\"text\"))\n",
    "            # response_dict = text_dict.get(\"response\")\n",
    "            # response_data[\"candidates\"][i][\"content\"][\"parts\"][j] = response_dict\n",
    "        \n",
    "    with open(path, \"w\", encoding=\"utf-8\") as f:\n",
    "        json.dump(response_data, f, indent=4, ensure_ascii=False)\n",
    "    \n",
    "    return response_data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Run"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "response = generate_analysis(\n",
    "    system_prompt=system_prompt,\n",
    "    user_message=user_message,\n",
    "    path_dir=Path(\n",
    "        \"/mnt/mydisk/Projects/plu/data/interim/Grenoble/Dispositions_Generales\"\n",
    "    ),\n",
    ")\n",
    "\n",
    "response"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "save_generation_response(response, Path(\"/mnt/mydisk/Projects/plu/data/processed/response_text.json\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "response.to_dict()[\"candidates\"][0][\"content\"][\"parts\"][0][\"text\"]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Conversion en PDF"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import markdown\n",
    "from weasyprint import HTML\n",
    "\n",
    "# Lire le fichier Markdown\n",
    "with open('/mnt/mydisk/Projects/plu/data/processed/plui-grenoble.md', 'r', encoding='utf-8') as f:\n",
    "    markdown_content = f.read()\n",
    "    \n",
    "# Convertir le Markdown en HTML\n",
    "html_content = markdown.markdown(markdown_content, extensions=['tables', 'fenced_code', 'codehilite', 'nl2br'])\n",
    "\n",
    "# Ajouter un style CSS basique\n",
    "html_content = f\"\"\"\n",
    "<!DOCTYPE html>\n",
    "<html>\n",
    "<head>\n",
    "    <meta charset=\"UTF-8\">\n",
    "    <style>\n",
    "        body {{ font-family: Arial, sans-serif; margin: 0cm; line-height: 1.5; }}\n",
    "        h1 {{ color: #2c3e50; }}\n",
    "        h2 {{ color: #3498db; margin-top: 1.5em; }}\n",
    "        h3 {{ color: #2980b9; margin-top: 1.2em; }}\n",
    "        blockquote {{ background-color: #f8f9fa; padding: 10px; border-left: 4px solid #3498db; margin-left: 10px; }}\n",
    "        li {{ margin-bottom: 8px; }}\n",
    "        code {{ background-color: #f8f9fa; padding: 2px 4px; }}\n",
    "    </style>\n",
    "</head>\n",
    "<body>\n",
    "    {html_content}\n",
    "</body>\n",
    "</html>\n",
    "\"\"\"\n",
    "\n",
    "# Générer le PDF\n",
    "HTML(string=html_content).write_pdf('/mnt/mydisk/Projects/plu/data/processed/plui-grenoble.pdf')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "plu",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
